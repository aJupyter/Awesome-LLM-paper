# Awesome-LLM-paper

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/aJupyter/Awesome-LLM-paper)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Made With Love](https://img.shields.io/badge/Made%20With-Love-red.svg)](https://github.com/aJupyter/Awesome-LLM-paper)

This repository contains papers related to all kinds of LLMs.

We strongly encourage researchers in the hope of advancing their excellent work.

---

## Contents

- [Awesome-LLM-paper](#awesome-llm-paper)
    - [Contents](#contents)
- [Resources](#resources)
    - [Workshops and Tutorials](#workshops-and-tutorials)
- [Papers](#papers)
    - [Survey](#survey)
    - [Benchmark and Evaluation](#benchmark-and-evaluation)
    - [RAG](#rag)
    - [Embedding](#embedding)
    - [LLM](#llm)
    - [Agent](#agent)
    - [MMLM](#mmlm)
- [ğŸŒŸ Contributors](#-contributors)
- [Star History](#star-history)

---

# Resources

## Workshops and Tutorials

<table>
    <tr>
        <th>Theme</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
</table>

# Papers

## Survey

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2311.12320">A Survey on Multimodal Large Language Models for Autonomous Driving</a></th>
        <th>arXiv:2311.12320</th>
        <th><a href="https://www.bilibili.com/video/BV14i4y1q74H/?spm_id_from=333.999.0.0">bilibili</a></th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2312.10997">Retrieval-Augmented Generation for Large Language Models: A Survey</a></th>
        <th>Arxiv2023'Tongji University</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">This paper provides a comprehensive overview of the integration of retrieval mechanisms with generative processes within large language models to enhance their performance and knowledge capabilities.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2311.12320">A Survey on Multimodal Large Language Models for Autonomous Driving</a></th>
        <th>WACV2023'Purdue University</th>
        <th><a href="https://www.bilibili.com/video/BV14i4y1q74H/">Bilibili: MLM for Autonomous Driving Survey</a></th>
        <th><a href="https://purduedigitaltwin.github.io/">Github: MLM for Autonomous Driving Survey</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">This paper provides a comprehensive overview of the integration of retrieval mechanisms with generative processes within large language models to enhance their performance and knowledge capabilities.</td>
    </tr>

</table>

## Benchmark and Evaluation

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
</table>

## RAG

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2401.00368">Improving Text Embeddings with Large Language Models</a></th>
        <th>Arxiv2024'Microsoft</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2311.09476">ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems</a></th>
        <th>NAACL 2024</th>
        <th><a href="https://www.bilibili.com/video/BV18H4y1W7GL/">bilibili</a></th>
        <th><a href="https://github.com/stanford-futuredata/ARES">Code: stanford-futuredata/ARES</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">ARES, an Automated RAG Evaluation System, efficiently evaluates retrieval-augmented generation systems across multiple tasks using synthetic data and minimal human annotations, maintaining accuracy even with domain shifts.</td>
    </tr>
</table>

## Embedding

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
       </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2309.07597">C-Pack: Packaged Resources To Advance General Chinese Embedding</a></th>
        <th>Arxiv2023'BAAI</th>
        <th><a href="https://www.bilibili.com/video/BV1VN4y1q7Zt/?spm_id_from=333.337.search-card.all.click&vd_source=7830a50943e3cd844c66dc7aca159592">Bilibili: C-Pack</a></th>
        <th><a href="https://github.com/FlagOpen/FlagEmbedding">Github: C-Pack</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">BAAI and Huggingface introduce C-Pack which is an advanced model for Chinese embeddings, significantly outperforming existing models and includes comprehensive benchmarks, a massive dataset, and a range of models.
BAAI è”åˆHuggingface æ¨å‡ºçš„ C-Packï¼Œä¸»æ‰“ä¸­æ–‡åµŒå…¥ï¼Œæ€§èƒ½æ˜æ˜¾ä¼˜äºç°æœ‰æ¨¡å‹ï¼ŒåŒ…æ‹¬å…¨é¢çš„åŸºå‡†æµ‹è¯•ã€å¤§è§„æ¨¡æ•°æ®é›†å’Œå¤šç§æ¨¡å‹ã€‚


</td>
    </tr>
</table>

## LLM

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
       </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></th>
        <th>Arxiv2023'Meta</th>
        <th><a href="https://www.bilibili.com/video/BV1YK411x7DF/?spm_id_from=333.337.search-card.all.click">bilibili</a></th>
        <th><a href="https://github.com/meta-llama/llama">Github: Llama</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">The technical report of Llama 2 from Meta Which is one of the top leaders of the LLMs open-sourced community. The greatest contribution of Llama 2 is the development of a range of pretrained and fine-tuned large language models (LLMs) that not only outperform existing open-source chat models on various benchmarks but are also optimized for dialogue scenarios. Additionally, these models have shown excellent performance in human evaluations of helpfulness and safety, potentially serving as effective substitutes for closed-source models. The Llama 2 project also provides a detailed description of the fine-tuning process and safety enhancements, aimed at fostering further development by the community and contributing to the responsible development of large language models.

æœ¬è®ºæ–‡æ˜¯Llama 2 æ¨¡å‹å‘å¸ƒçš„æŠ€æœ¯æŠ¥å‘Šï¼Œæ¥è‡ªå…¨çƒæœ€ä¸»è¦çš„å¤§æ¨¡å‹å¼€æºé¢†è¢–ä¹‹ä¸€ Metaã€‚Llama 2çš„æœ€å¤§è´¡çŒ®æ˜¯å¼€å‘äº†ä¸€ç³»åˆ—é¢„è®­ç»ƒå’Œå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¿™äº›æ¨¡å‹ä¸ä»…åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„å¼€æºèŠå¤©æ¨¡å‹ï¼Œè€Œä¸”è¿˜ç»è¿‡ä¼˜åŒ–ï¼Œç‰¹åˆ«é€‚ç”¨äºå¯¹è¯åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹åœ¨äººç±»è¯„ä¼°çš„å¸®åŠ©æ€§å’Œå®‰å…¨æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¯èƒ½æˆä¸ºé—­æºæ¨¡å‹çš„æœ‰æ•ˆæ›¿ä»£å“ã€‚Llama 2é¡¹ç›®è¿˜æä¾›äº†å¯¹å¾®è°ƒè¿‡ç¨‹å’Œå®‰å…¨æ€§æå‡çš„è¯¦ç»†æè¿°ï¼Œæ—¨åœ¨ä¿ƒè¿›ç¤¾åŒºåŸºäºæ­¤å·¥ä½œè¿›ä¸€æ­¥å‘å±•ï¼Œè´¡çŒ®äºè´Ÿè´£ä»»çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€å‘ã€‚ </td>
    </tr> 
    <tr>
        <th><a href="https://arxiv.org/abs/2402.08562">Higher Layers Need More LoRA Experts</a></th>
        <th>Arxiv2024'Northwestern University</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">In deep learning models, higher layers require more LoRA (Low-Rank Adaptation) experts to enhance the modelâ€™s expressive power and adaptability.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2310.06839">LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression</a></th>
        <th>Arxiv2023'Microsoft</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">To accelerate and enhance the performance of large language models (LLMs) in handling long texts, compressing prompts can be an effective method.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2401.13275">Can AI Assistants Know What They Don't Know?</a></th>
        <th>Arxiv2024'Fudan University</th>
        <th>â€¦â€¦</th>
        <th><a href="https://github.com/OpenMOSS/Say-I-Dont-Know">Code: Say-I-Dont-Know</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">The paper explores if AI assistants can identify when they don't know something, creating a "I don't know" dataset to teach this, resulting in fewer false answers and increased accuracy.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2308.12950">Code Llama: Open Foundation Models for Code</a></th>
        <th>Arxiv2023'Meta AI</th>
        <th><a href="https://www.bilibili.com/video/BV1YK411x7DF/?spm_id_from=333.788&vd_source=399f12e8692030e26d7132fc951d78d3">bilibili</a></th>
        <th><a href="https://github.com/meta-llama/codellama">codellama</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">The article introduces Code Llama, a family of large programming language models developed by Meta AI, based on Llama 2, designed to offer state-of-the-art performance among open models, support large input contexts, and possess zero-shot instruction following capabilities for programming tasks.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2304.15004">Are Emergent Abilities of Large Language Models a Mirage?</a></th>
        <th>NIPS2023'Stanford University</th>
        <th><a href="https://www.bilibili.com/video/BV1qK411e7aY/?spm_id_from=333.788&vd_source=399f12e8692030e26d7132fc951d78d3">bilibili</a></th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">The article challenges the notion that large language models (LLMs) exhibit "emergent abilities," suggesting that these abilities may be an artifact of the metrics chosen by researchers rather than inherent properties of the models themselves. Through mathematical modeling, empirical testing, and meta-analysis, the authors demonstrate that alternative metrics or improved statistical methods can eliminate the perception of emergent abilities, casting doubt on their existence as a fundamental aspect of scaling AI models.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2312.06109">Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models</a></th>
        <th>Arxiv2023'MEGVII Technology</th>
        <th>â€¦â€¦</th>
        <th><a href="https://varybase.github.io/">VaryBase</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">The article introduces Vary, a method for expanding the visual vocabulary of Large Vision-Language Models (LVLMs) to enhance dense and fine-grained visual perception capabilities for specific visual tasks, such as document-level OCR or chart understanding.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/1908.10084">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a></th>
        <th>Arxiv2019'UKP Lab</th>
        <th><a href="https://www.bilibili.com/video/BV1Eg4y1U7Nh/?spm_id_from=333.788&vd_source=399f12e8692030e26d7132fc951d78d3">bilibili</a></th>
        <th><a href="https://github.com/UKPLab/sentence-transformers">sentence-transformers</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">The paper introduces Sentence-BERT (SBERT), a modification of the BERT network that employs siamese and triplet network structures to produce semantically meaningful sentence embeddings that can be compared using cosine similarity, thereby significantly enhancing the efficiency of sentence similarity search and clustering tasks.</td>
    </tr>


</table>

## Fine-tuning

<table>
    <tr>
        <th><a href="https://arxiv.org/abs/2110.04366">Towards a Unified View of Parameter-Efficient Transfer Learning</a></th>
        <th>ICLR2022'Carnegie Mellon University</th>
        <th>â€¦â€¦</th>
        <th><a href="https://github.com/jxhe/unify-parameter-efficient-tuning">unify-parameter-efficient-tuning</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">This paper presents a unified framework for understanding and improving various parameter-efficient transfer learning methods by modifying specific hidden states in pre-trained models, defining a set of design dimensions to differentiate between methods, and experimentally demonstrating the framework's ability to identify important design choices in previous methods and instantiate new parameter-efficient tuning methods that are more effective with fewer parameters.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a></th>
        <th>NeurIPS2023'University of Washington</th>
        <th><a href="https://www.bilibili.com/video/BV1MH4y1Y7wW?p=6&vd_source=7830a50943e3cd844c66dc7aca159592">bilibili</a></th>
        <th><a href="https://github.com/artidoro/qlora">Github: QLoRA</a></th>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">This paper introduces QLoRA, a method for fine-tuning LLMs that significantly reduces memory usage. QLoRA achieves this by:

- Using a new data type called 4-bit NormalFloat (NF4) for weights, which is efficient for storing normally distributed weight values.
- Applying "double quantization" to compress the size of quantization constants. 
- Employing "paged optimizers" to manage memory spikes during training.

è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º QLoRA çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—é™ä½å†…å­˜ä½¿ç”¨é‡ã€‚QLoRA é€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°è¿™ä¸€ç‚¹ï¼š

- ä½¿ç”¨ä¸€ç§åä¸º 4-bit NormalFloat (NF4) çš„æ–°æ•°æ®ç±»å‹æ¥å­˜å‚¨æƒé‡ï¼Œè¯¥æ•°æ®ç±»å‹å¯¹å­˜å‚¨æœä»æ­£æ€åˆ†å¸ƒçš„æƒé‡å€¼éå¸¸æœ‰æ•ˆã€‚
- åº”ç”¨â€œåŒé‡åŒ–â€æ¥å‹ç¼©é‡åŒ–å¸¸æ•°çš„å°ºå¯¸ã€‚
- é‡‡ç”¨â€œåˆ†é¡µä¼˜åŒ–å™¨â€æ¥ç®¡ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜å³°å€¼ã€‚

è¿™äº›åˆ›æ–°ä½¿ QLoRA èƒ½å¤Ÿåœ¨å†…å­˜æœ‰é™çš„å•ä¸ª GPU (48GB) ä¸Šå¾®è°ƒå¤§å‹æ¨¡å‹ (ä¾‹å¦‚ï¼Œ65B å‚æ•°)ã€‚ è®­ç»ƒå‡ºçš„æ¨¡å‹åœ¨èŠå¤©æœºå™¨äººåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¿‡äº† ChatGPT ç­‰å…ˆå‰æ¨¡å‹çš„æ€§èƒ½ã€‚</td>
    </tr>
        <tr>
        <th><a href="https://arxiv.org/abs/2101.00190">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a></th>
        <th>ArXive2021'Stanford University</th>
        <th><a href="https://www.bilibili.com/video/BV1MH4y1Y7wW?p=2&vd_source=7830a50943e3cd844c66dc7aca159592">bilibili</a></th>
        <th>...</th>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">This paper introduces prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks. Unlike fine-tuning, which modifies all language model parameters, prefix-tuning keeps them frozen and optimizes a small continuous task-specific vector (called the prefix). This allows prefix-tuning to be more efficient than fine-tuning, especially in low-data settings.

è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œå‰ç¼€å¾®è°ƒâ€çš„è½»é‡çº§æ›¿ä»£å¾®è°ƒçš„æ–¹æ³•ï¼Œç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ã€‚ä¸å¾®è°ƒä¿®æ”¹æ‰€æœ‰è¯­è¨€æ¨¡å‹å‚æ•°ä¸åŒï¼Œå‰ç¼€å¾®è°ƒä¿æŒå‚æ•°å†»ç»“ï¼Œå¹¶ä¼˜åŒ–ä¸€ä¸ªå°çš„è¿ç»­ä»»åŠ¡ç‰¹å®šå‘é‡ (ç§°ä¸ºå‰ç¼€)ã€‚è¿™ä½¿å¾—å‰ç¼€å¾®è°ƒæ¯”å¾®è°ƒæ›´æœ‰æ•ˆï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®é‡è¾ƒå°çš„èƒŒæ™¯ä¸‹ã€‚</td>
    </tr>
</table>

## Prompt/Context

<table>
    </tr>
        <tr>
        <th><a href="https://arxiv.org/abs/2305.14160">Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning</a></th>
        <th>EMNLP2023'Peking University</th>
        <th><a href="https://www.bilibili.com/video/BV13w411G7HQ/?spm_id_from=333.337.search-card.all.click&vd_source=7830a50943e3cd844c66dc7aca159592">bilibili</a></th>
        <th><a href="https://github.com/lancopku/label-words-are-anchors">Github: ICL</th>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">This paper sheds light on the inner workings of in-context learning (ICL) in LLMs.  While ICL has shown promise in enabling LLMs to perform various tasks through demonstrations, the mechanism behind this learning has been unclear.  The authors investigate this mechanism through the lens of information flow and discover that labels in the demonstrations act as anchors.  These labels serve two key functions: 1) During initial processing, semantic information accumulates within the representations of these label words. 2) This consolidated information acts as a reference point for the LLMs' final predictions.  Based on these findings, the paper introduces three novel contributions: 1) An anchor re-weighting method to enhance ICL performance, 2) A demonstration compression technique to improve efficiency, and 3) An analysis framework to diagnose ICL errors in GPT2-XL.  The effectiveness of these contributions validates the proposed mechanism and paves the way for future research in ICL.

è¿™ç¯‡è®ºæ–‡é€šè¿‡ä¿¡æ¯æµè§†è§’æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹  (ICL) çš„å†…éƒ¨å·¥ä½œåŸç†ã€‚è™½ç„¶ ICL åœ¨é€šè¿‡æ¼”ç¤ºè®©å¤§å‹è¯­è¨€æ¨¡å‹æ‰§è¡Œå„ç§ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†å…¶èƒŒåçš„å­¦ä¹ æœºåˆ¶ä¸€ç›´ä¸æ¸…æ¥šã€‚ä½œè€…é€šè¿‡ä¿¡æ¯æµçš„è§†è§’ç ”ç©¶äº†è¿™ç§æœºåˆ¶ï¼Œå¹¶å‘ç°æ¼”ç¤ºä¸­çš„æ ‡ç­¾å……å½“é”šç‚¹ä½œç”¨ã€‚è¿™äº›æ ‡ç­¾å…·æœ‰ä¸¤ä¸ªå…³é”®åŠŸèƒ½ï¼š1) åœ¨åˆå§‹å¤„ç†è¿‡ç¨‹ä¸­ï¼Œè¯­ä¹‰ä¿¡æ¯ä¼šç´¯ç§¯åœ¨è¿™äº›æ ‡ç­¾è¯çš„è¡¨å¾ä¸­ã€‚2) è¿™ç§æ•´åˆçš„ä¿¡æ¯ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æœ€ç»ˆé¢„æµ‹çš„å‚è€ƒç‚¹ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œè®ºæ–‡æå‡ºäº†ä¸‰é¡¹åŸåˆ›è´¡çŒ®ï¼š1) æé«˜ ICL æ€§èƒ½çš„é”šç‚¹é‡æ–°åŠ æƒæ–¹æ³•ï¼Œ2) æé«˜æ¨ç†æ•ˆç‡çš„æ¼”ç¤ºå‹ç¼©æŠ€æœ¯ï¼Œ3) ç”¨äºè¯Šæ–­ GPT2-XL ä¸­ ICL é”™è¯¯çš„åˆ†ææ¡†æ¶ã€‚è¿™äº›è´¡çŒ®çš„æœ‰æ•ˆæ€§éªŒè¯äº†æ‰€æå‡ºçš„æœºåˆ¶ï¼Œå¹¶ä¸º ICL çš„æœªæ¥ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚ã€‚</td>
    </tr>

</table>


</table>

## Agent

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
</table>

## MMLM

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
</table>

## Reinforcement Learning

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2208.06193">Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning</a></th>
        <th>ICLR2023</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">Diffusion strategies, as a highly expressive class of policies, are used in offline reinforcement learning scenarios to improve learning efficiency and decision-making performance.</td>
    </tr>
</table>

# ğŸŒŸ Contributors

<a href="https://github.com/aJupyter/Awesome-LLM-paper/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=aJupyter/Awesome-LLM-paper" />
</a>

# Star History

[![Star History Chart](https://api.star-history.com/svg?repos=aJupyter/Awesome-LLM-paper&type=Date)](https://star-history.com/#aJupyter/Awesome-LLM-paper&Date)
