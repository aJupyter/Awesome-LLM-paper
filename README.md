# Awesome-LLM-paper

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/aJupyter/Awesome-LLM-paper)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Made With Love](https://img.shields.io/badge/Made%20With-Love-red.svg)](https://github.com/aJupyter/Awesome-LLM-paper)

This repository contains papers related to all kinds of LLMs.

We strongly encourage researchers in the hope of advancing their excellent work.

---

## Contents

- [Awesome-LLM-paper](#awesome-llm-paper)
    - [Contents](#contents)
- [Resources](#resources)
    - [Workshops and Tutorials](#workshops-and-tutorials)
- [Papers](#papers)
    - [Survey](#survey)
    - [Benchmark and Evaluation](#benchmark-and-evaluation)
    - [RAG](#rag)
    - [Embedding](#embedding)
    - [LLM](#llm)
    - [Agent](#agent)
    - [Tool Leanrning](#tool-learning)
    - [MMLM](#mmlm)
- [ğŸŒŸ Contributors](#-contributors)
- [Star History](#star-history)

---

# Resources

## Workshops and Tutorials

<table>
    <tr>
        <th>Theme</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
</table>

# Papers

## Survey

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2311.12320">A Survey on Multimodal Large Language Models for Autonomous Driving</a></th>
        <th>arXiv:2311.12320</th>
        <th><a href="https://www.bilibili.com/video/BV14i4y1q74H/?spm_id_from=333.999.0.0">bilibili</a></th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2312.10997">Retrieval-Augmented Generation for Large Language Models: A Survey</a></th>
        <th>Arxiv2023'Tongji University</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">This paper provides a comprehensive overview of the integration of retrieval mechanisms with generative processes within large language models to enhance their performance and knowledge capabilities.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2311.12320">A Survey on Multimodal Large Language Models for Autonomous Driving</a></th>
        <th>WACV2023'Purdue University</th>
        <th><a href="https://www.bilibili.com/video/BV14i4y1q74H/">Bilibili: MLM for Autonomous Driving Survey</a></th>
        <th><a href="https://purduedigitaltwin.github.io/">Github: MLM for Autonomous Driving Survey</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">This paper provides a comprehensive overview of the integration of retrieval mechanisms with generative processes within large language models to enhance their performance and knowledge capabilities.</td>
    </tr>

</table>

## Benchmark and Evaluation

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
</table>

## RAG

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2401.00368">Improving Text Embeddings with Large Language Models</a></th>
        <th>Arxiv2024'Microsoft</th>
        <th>â€¦â€¦</th>
        <th><a href="https://huggingface.co/intfloat/e5-mistral-7b-instruct">Hugging Face: e5-mistral-7b-instruct</a></th>
    </tr>
        <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">Mistral's primary work is achieved through a two-stage prompt process:

1. The first stage generates a pool of candidate tasks through brainstorming prompts.
2. The second stage synthesizes data from the pool of candidate tasks.

The author categorizes tasks into two major types - asymmetric tasks where the retrieved pair consists of a query and a document, differentiated by length into short-long matches, long-short matches, long-long matches, and short-short matches, with a classic example being the search engine scenario.

The paper indicates that large language models (LLMs) can significantly improve the quality of text embeddings, partly due to the synthetic data and partly due to the autoregressive capabilities of the LLMs. Moreover, it can streamline multi-stage embedding tasks into a single-stage fine-tuning (SFT) task, simplifying the training process.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2311.09476">ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems</a></th>
        <th>NAACL 2024</th>
        <th><a href="https://www.bilibili.com/video/BV18H4y1W7GL/">bilibili</a></th>
        <th><a href="https://github.com/stanford-futuredata/ARES">Code: stanford-futuredata/ARES</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">ARES, an Automated RAG Evaluation System, efficiently evaluates retrieval-augmented generation systems across multiple tasks using synthetic data and minimal human annotations, maintaining accuracy even with domain shifts.</td>
    </tr>
</table>

## Embedding

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
       </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2309.07597">C-Pack: Packaged Resources To Advance General Chinese Embedding</a></th>
        <th>Arxiv2023'BAAI</th>
        <th><a href="https://www.bilibili.com/video/BV1VN4y1q7Zt/?spm_id_from=333.337.search-card.all.click&vd_source=7830a50943e3cd844c66dc7aca159592">Bilibili: C-Pack</a></th>
        <th><a href="https://github.com/FlagOpen/FlagEmbedding">Github: C-Pack</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">BAAI and Huggingface introduce C-Pack which is an advanced model for Chinese embeddings, significantly outperforming existing models and includes comprehensive benchmarks, a massive dataset, and a range of models.
BAAI è”åˆHuggingface æ¨å‡ºçš„ C-Packï¼Œä¸»æ‰“ä¸­æ–‡åµŒå…¥ï¼Œæ€§èƒ½æ˜æ˜¾ä¼˜äºç°æœ‰æ¨¡å‹ï¼ŒåŒ…æ‹¬å…¨é¢çš„åŸºå‡†æµ‹è¯•ã€å¤§è§„æ¨¡æ•°æ®é›†å’Œå¤šç§æ¨¡å‹ã€‚


</td>
    </tr>
</table>

## LLM

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
       </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></th>
        <th>Arxiv2023'Meta</th>
        <th><a href="https://www.bilibili.com/video/BV1YK411x7DF/?spm_id_from=333.337.search-card.all.click">bilibili</a></th>
        <th><a href="https://github.com/meta-llama/llama">Github: Llama</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">The technical report of Llama 2 from Meta Which is one of the top leaders of the LLMs open-sourced community. The greatest contribution of Llama 2 is the development of a range of pretrained and fine-tuned large language models (LLMs) that not only outperform existing open-source chat models on various benchmarks but are also optimized for dialogue scenarios. Additionally, these models have shown excellent performance in human evaluations of helpfulness and safety, potentially serving as effective substitutes for closed-source models. The Llama 2 project also provides a detailed description of the fine-tuning process and safety enhancements, aimed at fostering further development by the community and contributing to the responsible development of large language models.

æœ¬è®ºæ–‡æ˜¯Llama 2 æ¨¡å‹å‘å¸ƒçš„æŠ€æœ¯æŠ¥å‘Šï¼Œæ¥è‡ªå…¨çƒæœ€ä¸»è¦çš„å¤§æ¨¡å‹å¼€æºé¢†è¢–ä¹‹ä¸€ Metaã€‚Llama 2çš„æœ€å¤§è´¡çŒ®æ˜¯å¼€å‘äº†ä¸€ç³»åˆ—é¢„è®­ç»ƒå’Œå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¿™äº›æ¨¡å‹ä¸ä»…åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„å¼€æºèŠå¤©æ¨¡å‹ï¼Œè€Œä¸”è¿˜ç»è¿‡ä¼˜åŒ–ï¼Œç‰¹åˆ«é€‚ç”¨äºå¯¹è¯åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹åœ¨äººç±»è¯„ä¼°çš„å¸®åŠ©æ€§å’Œå®‰å…¨æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¯èƒ½æˆä¸ºé—­æºæ¨¡å‹çš„æœ‰æ•ˆæ›¿ä»£å“ã€‚Llama 2é¡¹ç›®è¿˜æä¾›äº†å¯¹å¾®è°ƒè¿‡ç¨‹å’Œå®‰å…¨æ€§æå‡çš„è¯¦ç»†æè¿°ï¼Œæ—¨åœ¨ä¿ƒè¿›ç¤¾åŒºåŸºäºæ­¤å·¥ä½œè¿›ä¸€æ­¥å‘å±•ï¼Œè´¡çŒ®äºè´Ÿè´£ä»»çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€å‘ã€‚ </td>
    </tr> 
    <tr>
        <th><a href="https://arxiv.org/abs/2402.08562">Higher Layers Need More LoRA Experts</a></th>
        <th>Arxiv2024'Northwestern University</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">In deep learning models, higher layers require more LoRA (Low-Rank Adaptation) experts to enhance the modelâ€™s expressive power and adaptability.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2310.06839">LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression</a></th>
        <th>Arxiv2023'Microsoft</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">To accelerate and enhance the performance of large language models (LLMs) in handling long texts, compressing prompts can be an effective method.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2401.13275">Can AI Assistants Know What They Don't Know?</a></th>
        <th>Arxiv2024'Fudan University</th>
        <th>â€¦â€¦</th>
        <th><a href="https://github.com/OpenMOSS/Say-I-Dont-Know">Code: Say-I-Dont-Know</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">The paper explores if AI assistants can identify when they don't know something, creating a "I don't know" dataset to teach this, resulting in fewer false answers and increased accuracy.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2308.12950">Code Llama: Open Foundation Models for Code</a></th>
        <th>Arxiv2023'Meta AI</th>
        <th><a href="https://www.bilibili.com/video/BV1YK411x7DF/?spm_id_from=333.788&vd_source=399f12e8692030e26d7132fc951d78d3">bilibili</a></th>
        <th><a href="https://github.com/meta-llama/codellama">codellama</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">The article introduces Code Llama, a family of large programming language models developed by Meta AI, based on Llama 2, designed to offer state-of-the-art performance among open models, support large input contexts, and possess zero-shot instruction following capabilities for programming tasks.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2304.15004">Are Emergent Abilities of Large Language Models a Mirage?</a></th>
        <th>NIPS2023'Stanford University</th>
        <th><a href="https://www.bilibili.com/video/BV1qK411e7aY/?spm_id_from=333.788&vd_source=399f12e8692030e26d7132fc951d78d3">bilibili</a></th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">The article challenges the notion that large language models (LLMs) exhibit "emergent abilities," suggesting that these abilities may be an artifact of the metrics chosen by researchers rather than inherent properties of the models themselves. Through mathematical modeling, empirical testing, and meta-analysis, the authors demonstrate that alternative metrics or improved statistical methods can eliminate the perception of emergent abilities, casting doubt on their existence as a fundamental aspect of scaling AI models.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2312.06109">Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models</a></th>
        <th>Arxiv2023'MEGVII Technology</th>
        <th>â€¦â€¦</th>
        <th><a href="https://varybase.github.io/">VaryBase</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">The article introduces Vary, a method for expanding the visual vocabulary of Large Vision-Language Models (LVLMs) to enhance dense and fine-grained visual perception capabilities for specific visual tasks, such as document-level OCR or chart understanding.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/1908.10084">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a></th>
        <th>Arxiv2019'UKP Lab</th>
        <th><a href="https://www.bilibili.com/video/BV1Eg4y1U7Nh/?spm_id_from=333.788&vd_source=399f12e8692030e26d7132fc951d78d3">bilibili</a></th>
        <th><a href="https://github.com/UKPLab/sentence-transformers">sentence-transformers</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">The paper introduces Sentence-BERT (SBERT), a modification of the BERT network that employs siamese and triplet network structures to produce semantically meaningful sentence embeddings that can be compared using cosine similarity, thereby significantly enhancing the efficiency of sentence similarity search and clustering tasks.</td>
        <tr>
        <th><a href="https://arxiv.org/abs/2401.04088">Mixtral of Experts</a></th>
        <th>Arxiv2019'UKP Lab</th>
        <th>â€¦â€¦</th>
        <th><a href="https://github.com/mistralai/mistral-src">Mixtral of Experts</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">Mixtral is a model based on the Transformer architecture with two key differences:

1. Mixtral supports a full dense context length of up to 32,000 tokens;
2. It utilizes a Mixture of Experts (MoE) layer instead of the traditional feedforward network blocks.

The model is similar to the Mistral 7B architecture, but each layer includes eight feedforward units ("experts"). During processing, a routing network at each layer selects two "experts" to handle and merge the output for each token. Although only two expertsâ€™ data are processed per token, different experts may be chosen at each timestep. As a result, while each token has access to 47B parameters, only 13B active parameters are used during inference. Mixtral was trained with a context range of 32k tokens and has outperformed or matched the Llama 2 70B and GPT-3.5 in benchmarks, particularly excelling in mathematics, code generation, and multilingual tasks. Additionally, a specially tuned modelâ€”Mixtral 8x7B â€“ Instructâ€”has surpassed human benchmark models including GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B chat models.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/pdf/2404.04167">Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model</a></th>
        <th></th>
        <th></th>
        <th>https://github.com/Chinese-Tiny-LLM/Chinese-Tiny-LLM</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">æœ¬ç ”ç©¶ä»‹ç»äº†CT-LLMï¼Œä¸€ä¸ªä¼˜å…ˆè€ƒè™‘ä¸­æ–‡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½¿ç”¨12000äº¿æ ‡è®°çš„å¤§å‹è¯­æ–™åº“ï¼Œå…¶ä¸­8000äº¿ä¸ºä¸­æ–‡æ ‡è®°ã€‚CT-LLMé€šè¿‡ä»å¤´å¼€å§‹å¹¶ä¸»è¦ä½¿ç”¨ä¸­æ–‡æ•°æ®ï¼Œå±•ç°äº†åœ¨ç†è§£å’Œå¤„ç†ä¸­æ–‡æ–¹é¢çš„å“è¶Šèƒ½åŠ›ï¼ŒåŒæ—¶é€šè¿‡å¯¹é½æŠ€æœ¯è¿›ä¸€æ­¥æå‡ã€‚è¯¥æ¨¡å‹åœ¨CHC-Benchä¸Šè¡¨ç°å‡ºè‰²ï¼Œåœ¨ä¸­æ–‡ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨è‹±è¯­ä»»åŠ¡ä¸­çš„ç†Ÿç»ƒç¨‹åº¦ã€‚æœ¬ç ”ç©¶æŒ‘æˆ˜äº†ä¸»è¦ä½¿ç”¨è‹±è¯­è¯­æ–™åº“è®­ç»ƒLLMçš„ç°æœ‰æ–¹æ³•ï¼Œå¼€è¾Ÿäº†æ–°çš„è®­ç»ƒæ–¹æ³•è§†é‡ã€‚é€šè¿‡å¼€æºå®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹å’Œç›¸å…³èµ„æºï¼ˆå¦‚MAP-CCå’ŒCHC-Benchï¼‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä¿ƒè¿›å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„è¿›ä¸€æ­¥æ¢ç´¢å’Œåˆ›æ–°ï¼Œæ¨åŠ¨æ›´åŒ…å®¹å’Œå¤šåŠŸèƒ½çš„è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚</td>
    </tr>
        </tr>
        <tr>
        <th><a href="https://arxiv.org/pdf/2207.07061">Confident Adaptive Language Modeling</a></th>
        <th>Source</th>
        <th></th>
        <th>Other</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">è¿‘å¹´æ¥ï¼ŒåŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œä½†è¿™äº›è¿›æ­¥ä¼´éšç€æ¨¡å‹è§„æ¨¡å’Œæ¨ç†æ—¶é—´æˆæœ¬çš„å¢åŠ ã€‚å®é™…ä¸Šï¼ŒLLMç”Ÿæˆçš„åºåˆ—åŒ…å«ä¸åŒéš¾åº¦çº§åˆ«çš„ä»»åŠ¡ï¼Œä¸€äº›é¢„æµ‹éœ€è¦æ¨¡å‹çš„å…¨éƒ¨è®¡ç®—èƒ½åŠ›ï¼Œè€Œå…¶ä»–é¢„æµ‹åˆ™å¯ä»¥ç”¨è¾ƒå°‘çš„è®¡ç®—èµ„æºå®Œæˆã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªä¿¡è‡ªé€‚åº”è¯­è¨€å»ºæ¨¡ï¼ˆCALMï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ ¹æ®è¾“å…¥å’Œç”Ÿæˆæ—¶é—´æ­¥éª¤åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºã€‚æˆ‘ä»¬è§£å†³äº†æ—©æœŸé€€å‡ºè§£ç çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç½®ä¿¡åº¦è¡¡é‡æ ‡å‡†ã€å°†åºåˆ—çº§çº¦æŸè¿æ¥åˆ°æ¯ä¸ªtokençš„é€€å‡ºå†³ç­–ä»¥åŠå¤„ç†ç”±äºæ—©æœŸé€€å‡ºå¯¼è‡´çš„éšè—è¡¨ç¤ºç¼ºå¤±ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œå¯å°†è®¡ç®—é‡å‡å°‘è‡³3å€ã€‚</td>
    </tr>


</table>

## Fine-tuning

<table>
    <tr>
        <th><a href="https://arxiv.org/abs/2110.04366">Towards a Unified View of Parameter-Efficient Transfer Learning</a></th>
        <th>ICLR2022'Carnegie Mellon University</th>
        <th>â€¦â€¦</th>
        <th><a href="https://github.com/jxhe/unify-parameter-efficient-tuning">unify-parameter-efficient-tuning</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">This paper presents a unified framework for understanding and improving various parameter-efficient transfer learning methods by modifying specific hidden states in pre-trained models, defining a set of design dimensions to differentiate between methods, and experimentally demonstrating the framework's ability to identify important design choices in previous methods and instantiate new parameter-efficient tuning methods that are more effective with fewer parameters.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a></th>
        <th>NeurIPS2023'University of Washington</th>
        <th><a href="https://www.bilibili.com/video/BV1MH4y1Y7wW?p=6&vd_source=7830a50943e3cd844c66dc7aca159592">bilibili</a></th>
        <th><a href="https://github.com/artidoro/qlora">Github: QLoRA</a></th>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">This paper introduces QLoRA, a method for fine-tuning LLMs that significantly reduces memory usage. QLoRA achieves this by:

- Using a new data type called 4-bit NormalFloat (NF4) for weights, which is efficient for storing normally distributed weight values.
- Applying "double quantization" to compress the size of quantization constants. 
- Employing "paged optimizers" to manage memory spikes during training.

è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º QLoRA çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—é™ä½å†…å­˜ä½¿ç”¨é‡ã€‚QLoRA é€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°è¿™ä¸€ç‚¹ï¼š

- ä½¿ç”¨ä¸€ç§åä¸º 4-bit NormalFloat (NF4) çš„æ–°æ•°æ®ç±»å‹æ¥å­˜å‚¨æƒé‡ï¼Œè¯¥æ•°æ®ç±»å‹å¯¹å­˜å‚¨æœä»æ­£æ€åˆ†å¸ƒçš„æƒé‡å€¼éå¸¸æœ‰æ•ˆã€‚
- åº”ç”¨â€œåŒé‡åŒ–â€æ¥å‹ç¼©é‡åŒ–å¸¸æ•°çš„å°ºå¯¸ã€‚
- é‡‡ç”¨â€œåˆ†é¡µä¼˜åŒ–å™¨â€æ¥ç®¡ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜å³°å€¼ã€‚

è¿™äº›åˆ›æ–°ä½¿ QLoRA èƒ½å¤Ÿåœ¨å†…å­˜æœ‰é™çš„å•ä¸ª GPU (48GB) ä¸Šå¾®è°ƒå¤§å‹æ¨¡å‹ (ä¾‹å¦‚ï¼Œ65B å‚æ•°)ã€‚ è®­ç»ƒå‡ºçš„æ¨¡å‹åœ¨èŠå¤©æœºå™¨äººåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¿‡äº† ChatGPT ç­‰å…ˆå‰æ¨¡å‹çš„æ€§èƒ½ã€‚</td>
    </tr>
        <tr>
        <th><a href="https://arxiv.org/abs/2101.00190">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a></th>
        <th>ArXive2021'Stanford University</th>
        <th><a href="https://www.bilibili.com/video/BV1MH4y1Y7wW?p=2&vd_source=7830a50943e3cd844c66dc7aca159592">bilibili</a></th>
        <th>...</th>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">This paper introduces prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks. Unlike fine-tuning, which modifies all language model parameters, prefix-tuning keeps them frozen and optimizes a small continuous task-specific vector (called the prefix). This allows prefix-tuning to be more efficient than fine-tuning, especially in low-data settings.

è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œå‰ç¼€å¾®è°ƒâ€çš„è½»é‡çº§æ›¿ä»£å¾®è°ƒçš„æ–¹æ³•ï¼Œç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ã€‚ä¸å¾®è°ƒä¿®æ”¹æ‰€æœ‰è¯­è¨€æ¨¡å‹å‚æ•°ä¸åŒï¼Œå‰ç¼€å¾®è°ƒä¿æŒå‚æ•°å†»ç»“ï¼Œå¹¶ä¼˜åŒ–ä¸€ä¸ªå°çš„è¿ç»­ä»»åŠ¡ç‰¹å®šå‘é‡ (ç§°ä¸ºå‰ç¼€)ã€‚è¿™ä½¿å¾—å‰ç¼€å¾®è°ƒæ¯”å¾®è°ƒæ›´æœ‰æ•ˆï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®é‡è¾ƒå°çš„èƒŒæ™¯ä¸‹ã€‚</td>
    <tr>
        <th><a href="https://arxiv.org/abs/2110.07602">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</th>
        <th>ACL2022'Tsinghua University</th>
        <th>â€¦â€¦</th>
        <th><a href="https://github.com/THUDM/P-tuning-v2">Github: </th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">The author introduces P-tuning v2, which utilizes deep prompt optimization techniques, such as Prefix Tuning, to improve upon Prompt Tuning and P-Tuning as a universal solution across scales and NLU tasks. Compared to P-tuning, this method incorporates prompt tokens at every layer rather than just at the input layer, bringing two main benefits:

1. More learnable parameters (increasing from 0.01% in P-tuning and Prompt Tuning to 0.1%-3%), while still being efficient.
2. Embedding prompts into deeper structural layers has a more direct impact on model predictions. </td>
    </tr>
</table>

## Prompt/Context

<table>
    </tr>
        <tr>
        <th><a href="https://arxiv.org/abs/2305.14160">Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning</a></th>
        <th>EMNLP2023'Peking University</th>
        <th><a href="https://www.bilibili.com/video/BV13w411G7HQ/?spm_id_from=333.337.search-card.all.click&vd_source=7830a50943e3cd844c66dc7aca159592">bilibili</a></th>
        <th><a href="https://github.com/lancopku/label-words-are-anchors">Github: ICL</th>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">This paper sheds light on the inner workings of in-context learning (ICL) in LLMs.  While ICL has shown promise in enabling LLMs to perform various tasks through demonstrations, the mechanism behind this learning has been unclear.  The authors investigate this mechanism through the lens of information flow and discover that labels in the demonstrations act as anchors.  These labels serve two key functions: 1) During initial processing, semantic information accumulates within the representations of these label words. 2) This consolidated information acts as a reference point for the LLMs' final predictions.  Based on these findings, the paper introduces three novel contributions: 1) An anchor re-weighting method to enhance ICL performance, 2) A demonstration compression technique to improve efficiency, and 3) An analysis framework to diagnose ICL errors in GPT2-XL.  The effectiveness of these contributions validates the proposed mechanism and paves the way for future research in ICL.

è¿™ç¯‡è®ºæ–‡é€šè¿‡ä¿¡æ¯æµè§†è§’æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹  (ICL) çš„å†…éƒ¨å·¥ä½œåŸç†ã€‚è™½ç„¶ ICL åœ¨é€šè¿‡æ¼”ç¤ºè®©å¤§å‹è¯­è¨€æ¨¡å‹æ‰§è¡Œå„ç§ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†å…¶èƒŒåçš„å­¦ä¹ æœºåˆ¶ä¸€ç›´ä¸æ¸…æ¥šã€‚ä½œè€…é€šè¿‡ä¿¡æ¯æµçš„è§†è§’ç ”ç©¶äº†è¿™ç§æœºåˆ¶ï¼Œå¹¶å‘ç°æ¼”ç¤ºä¸­çš„æ ‡ç­¾å……å½“é”šç‚¹ä½œç”¨ã€‚è¿™äº›æ ‡ç­¾å…·æœ‰ä¸¤ä¸ªå…³é”®åŠŸèƒ½ï¼š1) åœ¨åˆå§‹å¤„ç†è¿‡ç¨‹ä¸­ï¼Œè¯­ä¹‰ä¿¡æ¯ä¼šç´¯ç§¯åœ¨è¿™äº›æ ‡ç­¾è¯çš„è¡¨å¾ä¸­ã€‚2) è¿™ç§æ•´åˆçš„ä¿¡æ¯ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æœ€ç»ˆé¢„æµ‹çš„å‚è€ƒç‚¹ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œè®ºæ–‡æå‡ºäº†ä¸‰é¡¹åŸåˆ›è´¡çŒ®ï¼š1) æé«˜ ICL æ€§èƒ½çš„é”šç‚¹é‡æ–°åŠ æƒæ–¹æ³•ï¼Œ2) æé«˜æ¨ç†æ•ˆç‡çš„æ¼”ç¤ºå‹ç¼©æŠ€æœ¯ï¼Œ3) ç”¨äºè¯Šæ–­ GPT2-XL ä¸­ ICL é”™è¯¯çš„åˆ†ææ¡†æ¶ã€‚è¿™äº›è´¡çŒ®çš„æœ‰æ•ˆæ€§éªŒè¯äº†æ‰€æå‡ºçš„æœºåˆ¶ï¼Œå¹¶ä¸º ICL çš„æœªæ¥ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚ã€‚</td>
    </tr>

</table>



## Ad click prediction 
</table>
    </tr>
        <tr>
        <th><a href="https://arxiv.org/pdf/1708.05123">Deep & Cross Network for Ad Click Predictions</a></th>
        <th>Source</th>
        <th></th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">ç‰¹å¾å·¥ç¨‹å¯¹äºé¢„æµ‹æ¨¡å‹çš„æˆåŠŸè‡³å…³é‡è¦ï¼Œä½†å¸¸å¸¸éœ€è¦æ‰‹åŠ¨æ“ä½œæˆ–è¯¦å°½æœç´¢ã€‚å°½ç®¡DNNèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ ç‰¹å¾äº¤äº’ï¼Œä½†åœ¨æ‰€æœ‰ç±»å‹çš„ç‰¹å¾äº¤äº’ä¸­æ•ˆç‡ä¸é«˜ã€‚æœ¬æ–‡æå‡ºäº†æ·±åº¦ä¸äº¤å‰ç½‘ç»œï¼ˆDCNï¼‰ï¼Œè¯¥ç½‘ç»œåœ¨ä¿ç•™DNNä¼˜åŠ¿çš„åŒæ—¶ï¼Œå¼•å…¥äº†é«˜æ•ˆå­¦ä¹ ç‰¹å®šç‰¹å¾äº¤äº’çš„æ–°å‹äº¤å‰ç½‘ç»œã€‚DCNåœ¨æ¯å±‚æ˜¾å¼è¿›è¡Œç‰¹å¾äº¤å‰ï¼Œæ— éœ€æ‰‹åŠ¨ç‰¹å¾å·¥ç¨‹ï¼Œå¹¶ä¸”å¢åŠ çš„å¤æ‚æ€§æå°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDCNåœ¨CTRé¢„æµ‹å’Œå¯†é›†åˆ†ç±»æ•°æ®é›†ä¸Šçš„æ¨¡å‹å‡†ç¡®æ€§å’Œå†…å­˜ä½¿ç”¨å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›ç®—æ³•ã€‚</td>
    </tr>

</table>

## Agent

<table>
    <tr>
        <th><a href="https://arxiv.org/pdf/2307.07924">ChatDev: Communicative Agents for Software Development</a></th>
        <th>Source</th>
        <th>https://github.com/OpenBMB/ChatDev</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">è½¯ä»¶å¼€å‘æ˜¯ä¸€ä¸ªéœ€è¦å¤šç§æŠ€èƒ½ååŒçš„å¤æ‚ä»»åŠ¡ã€‚ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨ç€‘å¸ƒæ¨¡å‹çš„ä¸åŒé˜¶æ®µï¼ˆå¦‚è®¾è®¡ã€ç¼–ç å’Œæµ‹è¯•ï¼‰ä¸­å­˜åœ¨æŠ€æœ¯ä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´å¼€å‘è¿‡ç¨‹ä½æ•ˆã€‚æœ¬æ–‡æå‡ºäº†ChatDevï¼Œä¸€ä¸ªç”±å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„èŠå¤©å¼è½¯ä»¶å¼€å‘æ¡†æ¶ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€å’Œç¼–ç¨‹è¯­è¨€çš„ç»Ÿä¸€æ²Ÿé€šæ–¹å¼ï¼Œä¿ƒè¿›å¤šä»£ç†ç³»ç»Ÿåœ¨è®¾è®¡ã€ç¼–ç å’Œæµ‹è¯•é˜¶æ®µçš„åä½œï¼Œæé«˜äº†å¼€å‘æ•ˆç‡ã€‚ChatDevé€šè¿‡å¤šè½®å¯¹è¯ç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†è¯­è¨€ä½œä¸ºå¤šä»£ç†åä½œçš„æ¡¥æ¢çš„æ½œåŠ›</td>

</table>

## Tool Learning

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
</table>

## MMLM

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
</table>

## Reinforcement Learning

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2208.06193">Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning</a></th>
        <th>ICLR2023</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">Diffusion strategies, as a highly expressive class of policies, are used in offline reinforcement learning scenarios to improve learning efficiency and decision-making performance.</td>
    </tr>
</table>
<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/pdf/2301.12050">Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling</a></th>
        <th>ICLR2023</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†é€šå¸¸æ²¡æœ‰å…ˆéªŒçŸ¥è¯†ï¼Œä»é›¶å¼€å§‹å­¦ä¹ ã€‚æˆ‘ä»¬æå‡ºä½¿ç”¨å°‘é‡æ ·æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å‡è®¾å¹¶éªŒè¯ä¸€ä¸ªæŠ½è±¡ä¸–ç•Œæ¨¡å‹ï¼ˆAWMï¼‰ï¼Œä»¥æé«˜RLä»£ç†çš„æ ·æœ¬æ•ˆç‡ã€‚DECKARDä»£ç†åœ¨Minecraftä¸­è¿›è¡Œç‰©å“åˆ¶ä½œï¼Œé€šè¿‡ä¸¤ä¸ªé˜¶æ®µå®ç°ï¼šDreamé˜¶æ®µï¼Œä»£ç†åˆ©ç”¨LLMå°†ä»»åŠ¡åˆ†è§£ä¸ºå­ç›®æ ‡å½¢æˆAWMï¼›Wakeé˜¶æ®µï¼Œä»£ç†ä¸ºæ¯ä¸ªå­ç›®æ ‡å­¦ä¹ ç­–ç•¥å¹¶éªŒè¯AWMã€‚è¿™ç§æ–¹æ³•ä¸ä»…æ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œè¿˜èƒ½çº æ­£LLMä¸­çš„é”™è¯¯ï¼ŒæˆåŠŸç»“åˆLLMsçš„å™ªå£°ä¿¡æ¯ä¸ç¯å¢ƒåŠ¨æ€ä¸­çš„çŸ¥è¯†ã€‚</td>
    </tr>
</table>


# ğŸŒŸ Contributors

<a href="https://github.com/aJupyter/Awesome-LLM-paper/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=aJupyter/Awesome-LLM-paper" />
</a>

# Star History

[![Star History Chart](https://api.star-history.com/svg?repos=aJupyter/Awesome-LLM-paper&type=Date)](https://star-history.com/#aJupyter/Awesome-LLM-paper&Date)
