# Awesome-LLM-paper

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/aJupyter/Awesome-LLM-paper)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Made With Love](https://img.shields.io/badge/Made%20With-Love-red.svg)](https://github.com/aJupyter/Awesome-LLM-paper)

This repository contains papers related to all kinds of LLMs.

We strongly encourage researchers in the hope of advancing their excellent work.

---

## Contents

- [Awesome-LLM-paper](#awesome-llm-paper)
    - [Contents](#contents)
- [Resources](#resources)
    - [Workshops and Tutorials](#workshops-and-tutorials)
- [Papers](#papers)
    - [Survey](#survey)
    - [Benchmark and Evaluation](#benchmark-and-evaluation)
    - [RAG](#rag)
    - [Embedding](#embedding)
    - [LLM](#llm)
    - [Agent](#agent)
    - [MMLM](#mmlm)
- [ðŸŒŸ Contributors](#-contributors)
- [Star History](#star-history)

---

# Resources

## Workshops and Tutorials

<table>
    <tr>
        <th>Theme</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
</table>

# Papers

## Survey

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2311.12320">A Survey on Multimodal Large Language Models for Autonomous Driving</a></th>
        <th>arXiv:2311.12320</th>
        <th><a href="https://www.bilibili.com/video/BV14i4y1q74H/?spm_id_from=333.999.0.0">bilibili</a></th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2312.10997">Retrieval-Augmented Generation for Large Language Models: A Survey</a></th>
        <th>Arxiv2023'Tongji University</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">This paper provides a comprehensive overview of the integration of retrieval mechanisms with generative processes within large language models to enhance their performance and knowledge capabilities.</td>
    </tr>
</table>

## Benchmark and Evaluation

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
</table>

## RAG

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2401.00368">Improving Text Embeddings with Large Language Models</a></th>
        <th>Arxiv2024'Microsoft</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2311.09476">ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems</a></th>
        <th>NAACL 2024</th>
        <th><a href="https://www.bilibili.com/video/BV18H4y1W7GL/">bilibili</a></th>
        <th><a href="https://github.com/stanford-futuredata/ARES">Code: stanford-futuredata/ARES</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">ARES, an Automated RAG Evaluation System, efficiently evaluates retrieval-augmented generation systems across multiple tasks using synthetic data and minimal human annotations, maintaining accuracy even with domain shifts.</td>
    </tr>
</table>

## Embedding

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
</table>

## LLM

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2402.08562">Higher Layers Need More LoRA Experts</a></th>
        <th>Arxiv2024'Northwestern University</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">In deep learning models, higher layers require more LoRA (Low-Rank Adaptation) experts to enhance the modelâ€™s expressive power and adaptability.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2310.06839">LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression</a></th>
        <th>Arxiv2023'Microsoft</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">To accelerate and enhance the performance of large language models (LLMs) in handling long texts, compressing prompts can be an effective method.</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2401.13275">Can AI Assistants Know What They Don't Know?</a></th>
        <th>Arxiv2024'Fudan University</th>
        <th>â€¦â€¦</th>
        <th><a href="https://github.com/OpenMOSS/Say-I-Dont-Know">Code: Say-I-Dont-Know</a></th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">The paper explores if AI assistants can identify when they don't know something, creating a "I don't know" dataset to teach this, resulting in fewer false answers and increased accuracy.</td>
    </tr>
</table>

## Agent

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
</table>

## MMLM

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">â€¦â€¦</td>
    </tr>
</table>

## Reinforcement Learning

<table>
    <tr>
        <th>Paper</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2208.06193">Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning</a></th>
        <th>ICLR2023</th>
        <th>â€¦â€¦</th>
        <th>â€¦â€¦</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">Diffusion strategies, as a highly expressive class of policies, are used in offline reinforcement learning scenarios to improve learning efficiency and decision-making performance.</td>
    </tr>
</table>

# ðŸŒŸ Contributors

<a href="https://github.com/aJupyter/Awesome-LLM-paper/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=aJupyter/Awesome-LLM-paper" />
</a>

# Star History

[![Star History Chart](https://api.star-history.com/svg?repos=aJupyter/Awesome-LLM-paper&type=Date)](https://star-history.com/#aJupyter/Awesome-LLM-paper&Date)
